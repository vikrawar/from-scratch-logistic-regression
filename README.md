# Logistic Regression from Scratch (NumPy)

A focused, step-by-step implementation of logistic regression using **NumPy**.
This notebook demonstrates the core logic of how models learn:
- How inputs interact with weights and bias
- How predictions are mapped to probabilities using the sigmoid function
- How binary cross-entropy quantifies error
- How gradient descent updates parameters over time

---

## ðŸ” What's Inside

- Intuitive explanation of logistic regression mechanics
- Custom implementation of sigmoid, loss, and cost functions
- Manual gradient descent for parameter updates
- Predictions and accuracy evaluation
- Visualization of training progress (cost curve)
- Clear structure and inline comments for learning and readability

---

## ðŸŽ¯ Why This Project

Logistic regression is a fundamental algorithm in machine learning, used for binary classification and one of the first places we introduce non-linearity through the sigmoid function. It also forms the conceptual foundation of neural networks. Every neuron, at its core, is just a logistic unit with weights, bias, and a non-linear transformation.
This notebook builds that understanding from the ground up, making every step visible and intuitive.

---

## ðŸ““ Notebook

> [Open the notebook](./Logistic%20Regression%20from%20Scratch.ipynb)

---

## âŽš YouTube tutorial
> https://www.youtube.com/watch?v=Oy-evZWy5b0
